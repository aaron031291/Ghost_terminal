#!/usr/bin/env python3
"""
Trust Predictor Module

Forecasts the trustworthiness of logic units, contributors, or outcomes based on
historical patterns, behavior, and contextual factors. Helps make risk-aware decisions
and powers governance modules.
"""

import os
import re
import json
import time
import uuid
import logging
import hashlib
import datetime
from typing import Dict, List, Tuple, Any, Optional, Union, Set
from enum import Enum, auto
from dataclasses import dataclass, field
from collections import defaultdict, Counter
import traceback

# Mock imports for external dependencies
try:
    from cryptography.fernet import Fernet
except ImportError:
    # Mock Fernet for cryptography
    class Fernet:
        def __init__(self, key):
            self.key = key
        
        def encrypt(self, data):
            if isinstance(data, str):
                data = data.encode()
            return b"MOCK_ENCRYPTED_" + data
        
        def decrypt(self, token):
            if token.startswith(b"MOCK_ENCRYPTED_"):
                return token[len(b"MOCK_ENCRYPTED_"):]
            return token

try:
    from flask import Flask, request, jsonify
    from werkzeug.exceptions import BadRequest
except ImportError:
    # Mock Flask and related components
    class Flask:
        def __init__(self, name):
            self.name = name
            self.routes = {}
        
        def route(self, route_str, **options):
            def decorator(f):
                self.routes[route_str] = f
                return f
            return decorator
        
        def run(self, host='0.0.0.0', port=5000, debug=False):
            print(f"Mock Flask running on {host}:{port}")
    
    class request:
        @staticmethod
        def get_json(*args, **kwargs):
            return {}
    
    def jsonify(data):
        return data
    
    class BadRequest(Exception):
        pass

# Local imports
try:
    from .utils.logging_utils import setup_logger
    from .utils.config_manager import ConfigManager
    from .models.trust_models import TrustModel, BayesianTrustModel, GraphTrustModel
    from .storage.trust_ledger import TrustLedger
    from .ethical_analyst import EthicalAnalyst
    from .anomaly_detector import AnomalyDetector
    from .diagnostics import Diagnostics
except ImportError:
    # Mock local imports
    def setup_logger(name, level=logging.INFO):
        logger = logging.getLogger(name)
        logger.setLevel(level)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger
    
    class ConfigManager:
        @staticmethod
        def get_config(section, key=None, default=None):
            configs = {
                "trust_predictor": {
                    "default_threshold": 0.7,
                    "storage_path": "/tmp/trust_data",
                    "model_type": "bayesian",
                    "update_interval": 3600
                }
            }
            if key:
                return configs.get(section, {}).get(key, default)
            return configs.get(section, default)
    
    class TrustModel:
        def __init__(self, config=None):
            self.config = config or {}
        
        def predict(self, features):
            return 0.8
        
        def update(self, features, outcome):
            pass
        
        def save(self, path):
            pass
        
        def load(self, path):
            pass
    
    class BayesianTrustModel(TrustModel):
        pass
    
    class GraphTrustModel(TrustModel):
        pass
    
    class TrustLedger:
        def __init__(self, storage_path):
            self.storage_path = storage_path
            self.records = []
        
        def add_record(self, record):
            self.records.append(record)
            return True
        
        def get_records(self, entity_id=None, limit=100):
            if entity_id:
                return [r for r in self.records if r.get('entity_id') == entity_id][:limit]
            return self.records[:limit]
    
    class EthicalAnalyst:
        def analyze(self, content, context=None):
            return {"score": 0.9, "flags": [], "explanation": "Mock ethical analysis"}
    
    class AnomalyDetector:
        def detect(self, data, context=None):
            return {"anomalies": [], "score": 0.05, "explanation": "No anomalies detected"}
    
    class Diagnostics:
        def get_metrics(self, entity_id, metric_type=None):
            return {"performance": 0.95, "reliability": 0.92, "errors": []}


# Constants
DEFAULT_TRUST_THRESHOLD = 0.7
TRUST_SCORE_DECAY_RATE = 0.01
MAX_HISTORY_ITEMS = 1000
CACHE_EXPIRY = 3600  # 1 hour

# Enums
class TrustLevel(Enum):
    """Enum representing different trust levels."""
    BLACKLISTED = auto()
    UNTRUSTED = auto()
    CAUTION = auto()
    TRUSTED = auto()
    HIGHLY_TRUSTED = auto()

class EntityType(Enum):
    """Enum representing different entity types that can be evaluated."""
    LOGIC_UNIT = auto()
    CONTRIBUTOR = auto()
    SYSTEM = auto()
    OUTCOME = auto()
    COMPONENT = auto()

# Data classes
@dataclass
class TrustFeatures:
    """Features used for trust prediction."""
    entity_id: str
    entity_type: EntityType
    historical_performance: float = 0.5
    ethical_score: float = 0.5
    anomaly_score: float = 0.0
    error_rate: float = 0.0
    age: float = 0.0
    complexity: float = 0.5
    context_alignment: float = 0.5
    additional_features: Dict[str, float] = field(default_factory=dict)

@dataclass
class TrustPrediction:
    """Result of a trust prediction."""
    entity_id: str
    entity_type: EntityType
    trust_score: float
    trust_level: TrustLevel
    confidence: float
    timestamp: float = field(default_factory=time.time)
    explanation: List[str] = field(default_factory=list)
    feature_importance: Dict[str, float] = field(default_factory=dict)
    prediction_id: str = field(default_factory=lambda: str(uuid.uuid4()))

class TrustPredictor:
    """
    Predicts trustworthiness of various entities based on historical data and contextual factors.
    
    The TrustPredictor assigns probabilistic trust scores to logic chunks, contributors,
    or systems based on various signals including code accuracy, contributor performance,
    ethical compliance, and behavioral anomalies.
    """
    
    def __init__(self, config_path=None):
        """
        Initialize the TrustPredictor with configuration.
        
        Args:
            config_path: Path to configuration file (optional)
        """
        self.logger = setup_logger("trust_predictor")
        self.config_manager = ConfigManager()
        
        # Load configuration
        self.config = self.config_manager.get_config("trust_predictor", default={})
        self.default_threshold = self.config.get("default_threshold", DEFAULT_TRUST_THRESHOLD)
        self.storage_path = self.config.get("storage_path", "/tmp/trust_data")
        
        # Initialize storage
        os.makedirs(self.storage_path, exist_ok=True)
        self.ledger = TrustLedger(self.storage_path)
        
        # Initialize components
        self.ethical_analyst = EthicalAnalyst()
        self.anomaly_detector = AnomalyDetector()
        self.diagnostics = Diagnostics()
        
        # Initialize model
        model_type = self.config.get("model_type", "bayesian")
        if model_type == "graph":
            self.model = GraphTrustModel(self.config)
        else:
            self.model = BayesianTrustModel(self.config)
            
        # Load model if available
        model_path = os.path.join(self.storage_path, f"{model_type}_model.pkl")
        try:
            self.model.load(model_path)
            self.logger.info(f"Loaded trust model from {model_path}")
        except Exception as e:
            self.logger.warning(f"Could not load model: {e}. Using new model.")
        
        # Cache for predictions
        self.prediction_cache = {}
        self.last_update_time = time.time()
        self.update_interval = self.config.get("update_interval", 3600)  # 1 hour
        
        self.logger.info("TrustPredictor initialized")
    
    def predict_trust(
        self, 
        entity_id: str, 
        entity_type: EntityType, 
        context: Dict[str, Any] = None
    ) -> TrustPrediction:
        """
        Predict the trustworthiness of an entity.
        
        Args:
            entity_id: Unique identifier for the entity
            entity_type: Type of entity (logic unit, contributor, etc.)
            context: Additional context for the prediction
            
        Returns:
            TrustPrediction object with score and explanation
        """
        self.logger.debug(f"Predicting trust for {entity_type.name} {entity_id}")
        
        # Check cache first
        cache_key = f"{entity_id}:{entity_type.name}"
        if cache_key in self.prediction_cache:
            cached_pred, cache_time = self.prediction_cache[cache_key]
            if time.time() - cache_time < CACHE_EXPIRY:
                self.logger.debug(f"Using cached prediction for {cache_key}")
                return cached_pred
        
        # Gather features
        features = self._gather_features(entity_id, entity_type, context)
        
        # Make prediction
        trust_score = self.model.predict(features)
        
        # Determine trust level
        trust_level = self._score_to_level(trust_score)
        
        # Create prediction object
        prediction = TrustPrediction(
            entity_id=entity_id,
            entity_type=entity_type,
            trust_score=trust_score,
            trust_level=trust_level,
            confidence=0.8,  # TODO: Calculate actual confidence
            explanation=self._generate_explanation(features, trust_score),
            feature_importance=self._calculate_feature_importance(features)
        )
        
        # Cache the prediction
        self.prediction_cache[cache_key] = (prediction, time.time())
        
        # Store in ledger
        self._record_prediction(prediction, features)
        
        return prediction
    
    def batch_predict(
        self, 
        entities: List[Tuple[str, EntityType]], 
        context: Dict[str, Any] = None
    ) -> List[TrustPrediction]:
        """
        Predict trust for multiple entities at once.
        
        Args:
            entities: List of (entity_id, entity_type) tuples
            context: Shared context for all predictions
            
        Returns:
            List of TrustPrediction objects
        """
        return [self.predict_trust(eid, etype, context) for eid, etype in entities]
    
    def update_model(self, training_data: Optional[List[Dict[str, Any]]] = None) -> bool:
        """
        Update the trust model with new data.
        
        Args:
            training_data: Optional explicit training data, otherwise uses ledger
            
        Returns:
            True if update was successful
        """
        current_time = time.time()
        if current_time - self.last_update_time < self.update_interval:
            self.logger.debug("Skipping model update, not enough time elapsed")
            return False
        
        try:
            if not training_data:
                # Get recent records from ledger
                records = self.ledger.get_records(limit=MAX_HISTORY_ITEMS)
                if not records:
                    self.logger.warning("No records available for model update")
                    return False
                
                # Convert to training data
                training_data = []
                for record in records:
                    if 'features' in record and 'outcome' in record:
                        training_data.append({
                            'features': record['features'],
                            'outcome': record['outcome']
                        })
            
            # Update model
            for item in training_data:
                self.model.update(item['features'], item['outcome'])
            
            # Save updated model
            model_path = os.path.join(self.storage_path, f"{self.config.get('model_type', 'bayesian')}_model.pkl")
            self.model.save(model_path)
            
            self.last_update_time = current_time
            self.logger.info(f"Trust model updated with {len(training_data)} records")
            return True
            
        except Exception as e:
            self.logger.error(f"Error updating trust model: {e}")
            self.logger.error(traceback.format_exc())
            return False
    
    def record_outcome(
        self, 
        prediction_id: str, 
        actual_outcome: float, 
        metadata: Dict[str, Any] = None
    ) -> bool:
        """
        Record the actual outcome for a previous prediction.
        
        Args:
            prediction_id: ID of the prediction
            actual_outcome: Actual trust outcome (0-1)
            metadata: Additional metadata about the outcome
            
        Returns:
            True if successfully recorded
        """
        try:
            # Find the prediction record
            records = self.ledger.get_records()
            prediction_record = None
            
            for record in records:
                if record.get('prediction', {}).get('prediction_id') == prediction_id:
                    prediction_record = record
                    break
            
            if not prediction_record:
                self.logger.warning(f"No prediction found with ID {prediction_id}")
                return False
            
            # Update the record with the outcome
            prediction_record['outcome'] = actual_outcome
            prediction_record['outcome_metadata'] = metadata or {}
            prediction_record['outcome_timestamp'] = time.time()
            
            # Calculate prediction error
            predicted_score = prediction_record.get('prediction', {}).get('trust_score', 0.5)
            prediction_record['prediction_error'] = abs(predicted_score - actual_outcome)
            
            # Save updated record
            self.ledger.add_record(prediction_record)
            
            self.logger.info(f"Recorded outcome for prediction {prediction_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error recording outcome: {e}")
            return False
    
    def get_entity_trust_history(
        self, 
        entity_id: str, 
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        Get historical trust predictions for an entity.
        
        Args:
            entity_id: ID of the entity
            limit: Maximum number of records to return
            
        Returns:
            List of historical trust predictions
        """
        records = self.ledger.get_records(entity_
            entity_id=entity_id, limit=limit)
        
        # Extract just the prediction part
        history = []
        for record in records:
            if 'prediction' in record:
                history.append({
                    'prediction': record['prediction'],
                    'timestamp': record.get('timestamp', 0),
                    'outcome': record.get('outcome'),
                    'outcome_timestamp': record.get('outcome_timestamp')
                })
        
        # Sort by timestamp, newest first
        history.sort(key=lambda x: x['timestamp'], reverse=True)
        return history[:limit]
    
    def get_trust_statistics(self, entity_type: Optional[EntityType] = None) -> Dict[str, Any]:
        """
        Get aggregate statistics about trust predictions.
        
        Args:
            entity_type: Optional filter by entity type
            
        Returns:
            Dictionary of statistics
        """
        records = self.ledger.get_records(limit=1000)
        
        # Filter by entity type if specified
        if entity_type:
            records = [r for r in records if r.get('prediction', {}).get('entity_type') == entity_type.name]
        
        if not records:
            return {
                'count': 0,
                'average_trust': 0,
                'distribution': {},
                'accuracy': None
            }
        
        # Calculate statistics
        trust_scores = [r.get('prediction', {}).get('trust_score', 0) for r in records]
        trust_levels = [r.get('prediction', {}).get('trust_level') for r in records]
        
        # Count predictions with outcomes
        predictions_with_outcomes = [r for r in records if 'outcome' in r]
        if predictions_with_outcomes:
            errors = [abs(r.get('prediction', {}).get('trust_score', 0) - r.get('outcome', 0)) 
                     for r in predictions_with_outcomes]
            avg_error = sum(errors) / len(errors)
            accuracy = 1 - avg_error
        else:
            accuracy = None
        
        # Count distribution of trust levels
        level_counts = Counter(trust_levels)
        
        return {
            'count': len(records),
            'average_trust': sum(trust_scores) / len(trust_scores) if trust_scores else 0,
            'distribution': {level: count for level, count in level_counts.items()},
            'accuracy': accuracy
        }
    
    def _gather_features(
        self, 
        entity_id: str, 
        entity_type: EntityType, 
        context: Optional[Dict[str, Any]] = None
    ) -> TrustFeatures:
        """
        Gather all features needed for trust prediction.
        
        Args:
            entity_id: ID of the entity
            entity_type: Type of entity
            context: Additional context
            
        Returns:
            TrustFeatures object
        """
        context = context or {}
        
        # Get historical performance
        historical_performance = self._get_historical_performance(entity_id, entity_type)
        
        # Get ethical analysis if content is provided
        ethical_score = 0.5
        if 'content' in context:
            ethical_analysis = self.ethical_analyst.analyze(context['content'], context)
            ethical_score = ethical_analysis.get('score', 0.5)
        
        # Get anomaly score
        anomaly_score = 0.0
        if 'behavior_data' in context:
            anomaly_result = self.anomaly_detector.detect(context['behavior_data'], context)
            anomaly_score = anomaly_result.get('score', 0.0)
        
        # Get diagnostics metrics
        metrics = self.diagnostics.get_metrics(entity_id)
        error_rate = 1.0 - metrics.get('reliability', 0.5)
        
        # Calculate age (normalized)
        age = 0.0
        if 'creation_time' in context:
            age_in_days = (time.time() - context['creation_time']) / (24 * 3600)
            age = min(1.0, age_in_days / 365)  # Normalize to 0-1 over a year
        
        # Calculate complexity
        complexity = context.get('complexity', 0.5)
        
        # Calculate context alignment
        context_alignment = self._calculate_context_alignment(entity_id, context)
        
        # Additional features
        additional_features = {}
        for key, value in context.items():
            if key not in ['content', 'behavior_data', 'creation_time', 'complexity'] and isinstance(value, (int, float)):
                additional_features[key] = value
        
        return TrustFeatures(
            entity_id=entity_id,
            entity_type=entity_type,
            historical_performance=historical_performance,
            ethical_score=ethical_score,
            anomaly_score=anomaly_score,
            error_rate=error_rate,
            age=age,
            complexity=complexity,
            context_alignment=context_alignment,
            additional_features=additional_features
        )
    
    def _get_historical_performance(self, entity_id: str, entity_type: EntityType) -> float:
        """
        Calculate historical performance score based on past records.
        
        Args:
            entity_id: ID of the entity
            entity_type: Type of entity
            
        Returns:
            Performance score from 0 to 1
        """
        records = self.ledger.get_records(entity_id=entity_id, limit=100)
        
        if not records:
            return 0.5  # Default for unknown entities
        
        # Get records with outcomes
        records_with_outcomes = [r for r in records if 'outcome' in r]
        
        if not records_with_outcomes:
            # If no outcomes, use the average prediction
            predictions = [r.get('prediction', {}).get('trust_score', 0.5) for r in records]
            return sum(predictions) / len(predictions)
        
        # Calculate weighted average of outcomes, giving more weight to recent outcomes
        total_weight = 0
        weighted_sum = 0
        
        for i, record in enumerate(records_with_outcomes):
            # More recent records have higher weight
            weight = 1.0 / (1 + i * 0.1)
            outcome = record.get('outcome', 0.5)
            
            weighted_sum += outcome * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.5
    
    def _calculate_context_alignment(self, entity_id: str, context: Dict[str, Any]) -> float:
        """
        Calculate how well the entity aligns with the current context.
        
        Args:
            entity_id: ID of the entity
            context: Current context
            
        Returns:
            Alignment score from 0 to 1
        """
        # This is a simplified implementation
        # In a real system, this would compare the entity's expected context with current context
        
        if 'expected_context' in context:
            expected = set(context.get('expected_context', {}).keys())
            actual = set(context.keys())
            
            if not expected:
                return 0.5
            
            # Jaccard similarity between expected and actual context keys
            intersection = len(expected.intersection(actual))
            union = len(expected.union(actual))
            
            return intersection / union if union > 0 else 0.5
        
        return 0.5  # Default when no expected context is provided
    
    def _score_to_level(self, score: float) -> TrustLevel:
        """
        Convert a numerical trust score to a trust level.
        
        Args:
            score: Trust score from 0 to 1
            
        Returns:
            TrustLevel enum value
        """
        if score < 0.2:
            return TrustLevel.BLACKLISTED
        elif score < 0.4:
            return TrustLevel.UNTRUSTED
        elif score < 0.7:
            return TrustLevel.CAUTION
        elif score < 0.9:
            return TrustLevel.TRUSTED
        else:
            return TrustLevel.HIGHLY_TRUSTED
    
    def _generate_explanation(self, features: TrustFeatures, score: float) -> List[str]:
        """
        Generate human-readable explanations for a trust prediction.
        
        Args:
            features: Features used for prediction
            score: Resulting trust score
            
        Returns:
            List of explanation strings
        """
        explanations = []
        
        # Add explanations based on significant features
        if features.historical_performance > 0.8:
            explanations.append("Strong historical performance")
        elif features.historical_performance < 0.3:
            explanations.append("Poor historical performance")
        
        if features.ethical_score < 0.4:
            explanations.append("Ethical concerns detected")
        
        if features.anomaly_score > 0.6:
            explanations.append("Significant anomalies detected")
        
        if features.error_rate > 0.3:
            explanations.append(f"High error rate: {features.error_rate:.2f}")
        
        # Add general explanation about the score
        if score > 0.8:
            explanations.append("Overall high trust based on combined factors")
        elif score < 0.3:
            explanations.append("Overall low trust based on combined factors")
        else:
            explanations.append("Moderate trust level with some concerns")
        
        return explanations
    
    def _calculate_feature_importance(self, features: TrustFeatures) -> Dict[str, float]:
        """
        Calculate the importance of each feature in the prediction.
        
        Args:
            features: Features used for prediction
            
        Returns:
            Dictionary mapping feature names to importance scores
        """
        # This is a simplified implementation
        # In a real system, this would use model-specific feature importance
        
        # Default importance weights
        importance = {
            "historical_performance": 0.3,
            "ethical_score": 0.2,
            "anomaly_score": 0.15,
            "error_rate": 0.15,
            "age": 0.05,
            "complexity": 0.05,
            "context_alignment": 0.1
        }
        
        # Add any additional features with low importance
        for key in features.additional_features:
            importance[key] = 0.01
        
        # Normalize to ensure sum is 1.0
        total = sum(importance.values())
        return {k: v / total for k, v in importance.items()}
    
    def _record_prediction(self, prediction: TrustPrediction, features: TrustFeatures) -> bool:
        """
        Record a prediction in the ledger.
        
        Args:
            prediction: The prediction to record
            features: Features used for the prediction
            
        Returns:
            True if successfully recorded
        """
        record = {
            "entity_id": prediction.entity_id,
            "entity_type": prediction.entity_type.name,
            "timestamp": prediction.timestamp,
            "prediction": {
                "prediction_id": prediction.prediction_id,
                "trust_score": prediction.trust_score,
                "trust_level": prediction.trust_level.name,
                "confidence": prediction.confidence,
                "explanation": prediction.explanation,
                "feature_importance": prediction.feature_importance
            },
            "features": {
                "historical_performance": features.historical_performance,
                "ethical_score": features.ethical_score,
                "anomaly_score": features.anomaly_score,
                "error_rate": features.error_rate,
                "age": features.age,
                "complexity": features.complexity,
                "context_alignment": features.context_alignment,
                "additional_features": features.additional_features
            }
        }
        
        return self.ledger.add_record(record)


class TrustPredictorAPI:
    """API wrapper for the TrustPredictor."""
    
    def __init__(self, predictor=None):
        """
        Initialize the API with a TrustPredictor instance.
        
        Args:
            predictor: TrustPredictor instance (creates new one if None)
        """
        self.predictor = predictor or TrustPredictor()
        self.app = Flask("trust_predictor_api")
        self._setup_routes()
    
    def _setup_routes(self):
        """Set up the API routes."""
        
        @self.app.route('/predict', methods=['POST'])
        def predict():
            try:
                data = request.get_json(force=True)
                
                if not data or 'entity_id' not in data or 'entity_type' not in data:
                    raise BadRequest("Missing required fields: entity_id, entity_type")
                
                entity_id = data['entity_id']
                entity_type_str = data['entity_type'].upper()
                context = data.get('context', {})
                
                try:
                    entity_type = EntityType[entity_type_str]
                except KeyError:
                    valid_types = [e.name for e in EntityType]
                    raise BadRequest(f"Invalid entity_type. Must be one of: {valid_types}")
                
                prediction = self.predictor.predict_trust(entity_id, entity_type, context)
                
                return jsonify({
                    "prediction_id": prediction.prediction_id,
                    "entity_id": prediction.entity_id,
                    "entity_type": prediction.entity_type.name,
                    "trust_score": prediction.trust_score,
                    "trust_level": prediction.trust_level.name,
                    "confidence": prediction.confidence,
                    "timestamp": prediction.timestamp,
                    "explanation": prediction.explanation,
                    "feature_importance": prediction.feature_importance
                })
                
            except BadRequest as e:
                return jsonify({"error": str(e)}), 400
            except Exception as e:
                self.predictor.logger.error(f"Error in predict endpoint: {e}")
                return jsonify({"error": "Internal server error"}), 500
        
        @self.app.route('/batch_predict', methods=['POST'])
        def batch_predict():
            try:
                data = request.get_json(force=True)
                
                if not data or 'entities' not in data or not isinstance(data['entities'], list):
                    raise BadRequest("Missing or invalid 'entities' field")
                
                entities = []
                for entity in data['entities']:
                    if 'entity_id' not in entity or 'entity_type' not in entity:
                        raise BadRequest("Each entity must have entity_id and entity_type")
                    
                    entity_id = entity['entity_id']
                    entity_type_str = entity['entity_type'].upper()
                    
                    try:
                        entity_type = EntityType[entity_type_str]
                    except KeyError:
                        valid_types = [e.name for e in EntityType]
                        raise BadRequest(f"Invalid entity_type. Must be one of: {valid_types}")
                    
                    entities.append((entity_id, entity_type))
                
                context = data.get('context', {})
                predictions = self.predictor.batch_predict(entities, context)
                
                return jsonify({
                    "predictions": [
                        {
                            "prediction_id": p.prediction_id,
                            "entity_id": p.entity_id,
                            "entity_type": p.entity_type.name,
                            "trust_score": p.trust_score,
                            "trust_level": p.trust_level.name,
                            "confidence": p.confidence,
                            "timestamp": p.timestamp,
                            "explanation": p.explanation,
                            "feature_importance": p.feature_importance
                        }
                        for p in predictions
                    ]
                })
                
            except BadRequest as e:
                return jsonify({"error": str(e)}), 400
            except Exception as e:
                self.predictor.logger.error(f"Error in batch_predict endpoint: {e}")
                return jsonify({"error": "Internal server error"}), 500
        
        @self.app.route('/record_outcome', methods=['POST'])
        def record_outcome():
            try:
                data = request.get_json(force=True)
                
                if not data or 'prediction_id' not in data or 'outcome' not in data:
                    raise BadRequest("Missing required fields: prediction_id, outcome")
                
                prediction_id = data['prediction_id']
                outcome = float(data['outcome'])
                metadata = data.get('metadata', {})
                
                if outcome < 0 or outcome > 1:
                    raise BadRequest("Outcome must be between 0 and 1")
                
                success = self.predictor.record_outcome(prediction_id, outcome, metadata)
                
                if not success:
                    return jsonify({"error": "Failed to record outcome"}), 404
                
                return jsonify({"success": True})
                
            except BadRequest as e:
                return jsonify({"error": str(e)}), 400
            except ValueError:
                return jsonify({"error": "Outcome must be a valid number between 0 and 1"}), 400
            except Exception as e:
                self.predictor.logger.error(f"Error in record_outcome endpoint: {e}")
                return jsonify({"error": "Internal server error"}), 500
        
        @self.app.route('/history/<entity_id>', methods=['GET'])
        def get_history(entity_id):
            try:
                limit = request.args.get('limit', default=100, type=int)
                history = self.predictor.get_entity_trust_history(entity_id, limit)
                
                return jsonify({
                    "entity_id": entity_id,
                    "history": history
                })
                
            except Exception as e:
                self.predictor.logger.error(f"Error in get_history endpoint: {e}")
                return jsonify({"error": "Internal server error"}), 500
        
        @self.app.route('/statistics', methods=['GET'])
        def get_statistics():
            try:
                entity_type_str = request.args.get('entity_type')
                entity_type = None
                
                if entity_type_str:
                    try:
                        entity_type = EntityType[entity_type_str.upper()]
                    except KeyError:
                        valid_types = [e.name for e in EntityType]
                        raise BadRequest(f"Invalid entity_type. Must be one of: {valid_types}")
                
                stats = self.predictor.get_trust_statistics(entity_type)
                
                return jsonify(stats)
                
            except BadRequest as e:
                return jsonify({"error": str(e)}), 400
            except Exception as e:
                self.predictor.logger.error(f"Error in get_statistics endpoint: {e}")
                return jsonify({"error": "Internal server error"}), 500
        
        @self.app.route('/update_model', methods=['POST'])
        def update_model():
            try:
                data = request.get_json(force=True)
                training_data = data.get('training_data')
                
                success = self.predictor.update_model(training_data)
                
                return jsonify({"success": success})
                
            except Exception as e:
                self.predictor.logger.error(f"Error in update_model endpoint: {e}")
                return jsonify({"error": "Internal server error"}), 500
    
    def run(self, host='0.0.0.0', port=5000, debug=False):
        """
        Run the API server.
        
        Args:
            host: Host to bind to
            port: Port to bind to
            debug: Whether to run in debug mode
        """
        self.app.run(host=host, port=port, debug=debug)


def create_trust_predictor(config_path=None):
    """
    Factory function to create a TrustPredictor instance.
    
    Args:
        config_path: Path to configuration file
        
    Returns:
        TrustPredictor instance
    """
    return TrustPredictor(config_path)


if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Create predictor and API
    predictor = create_trust_predictor()
    api = TrustPredictorAPI(predictor)
    
    # Run API server
    api.run(debug=True)
